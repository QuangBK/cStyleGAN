{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "from UTKFaceDataset import UTKFaceDataset\n",
    "\n",
    "from model_face_UTK import StyledGenerator, Discriminator\n",
    "\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n",
    "\n",
    "\n",
    "def sample_data(PATH_IMG, batch_size, image_size=32):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = UTKFaceDataset(PATH_IMG, transform=transform)\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=16)\n",
    "\n",
    "    return loader\n",
    "\n",
    "def adjust_lr(optimizer, lr):\n",
    "    for group in optimizer.param_groups:\n",
    "        mult = group.get('mult', 1)\n",
    "        group['lr'] = lr * mult\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_size = 512\n",
    "batch_size = 16\n",
    "n_critic = 1\n",
    "\n",
    "class Args:\n",
    "    n_gpu = 4\n",
    "    phase = 600_000\n",
    "    lr = 0.001\n",
    "    init_size = 8\n",
    "    max_size = 128\n",
    "    mixing = False\n",
    "    loss = 'wgan-gp'\n",
    "    data = 'folder'\n",
    "    path = '/home/quang/working/dataset/dataFace/'\n",
    "    sched = None\n",
    "    \n",
    "args = Args()\n",
    "# args = {'n_gpu': 4, '': 600_000, 'lr': 0.001, 'init_size': 64, 'max_size': 1024, 'mixing': False, 'loss': 'wgan-gp', 'data': 'folder'}\n",
    "\n",
    "generator = nn.DataParallel(StyledGenerator(code_size)).cuda()\n",
    "discriminator = nn.DataParallel(Discriminator()).cuda()\n",
    "\n",
    "class_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "g_optimizer = optim.Adam(\n",
    "    generator.module.generator.parameters(), lr=args.lr, betas=(0.0, 0.99)\n",
    ")    \n",
    "g_optimizer.add_param_group(\n",
    "    {\n",
    "        'params': generator.module.style.parameters(),\n",
    "        'lr': args.lr * 0.01,\n",
    "        'mult': 0.01,\n",
    "    }\n",
    ")\n",
    "\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
    "\n",
    "if args.sched:\n",
    "    args.lr = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}\n",
    "    args.batch = {4: 512, 8: 256, 16: 128, 32: 64, 64: 32, 128: 32, 256: 32}\n",
    "\n",
    "else:\n",
    "    args.lr = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}\n",
    "    args.batch = {4: 32, 8: 32, 16: 32, 32: 32, 64: 32, 128: 16, 256: 8}\n",
    "\n",
    "args.gen_sample = {512: (8, 4), 1024: (4, 2)}\n",
    "\n",
    "args.batch_default = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "\n",
    "# checkpoint = torch.load('./save_model/train_step-4-159999_128.model')\n",
    "# generator.module.load_state_dict(checkpoint['generator'])\n",
    "# discriminator.module.load_state_dict(checkpoint['discriminator'])\n",
    "# g_optimizer.load_state_dict(checkpoint['g_optimizer'])\n",
    "# d_optimizer.load_state_dict(checkpoint['d_optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution:  128 |Step:  4 |Batch_size:  16  |Generator lr:  0.0015  |Style lr:  1.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Size: 128; G: 0.636; D: 0.653; Grad: 0.046; Alpha: 1.00000:  24%|██▎       | 118064/500000 [30:04:16<99:59:36,  1.06it/s]"
     ]
    }
   ],
   "source": [
    "step = int(math.log2(args.init_size)) - 3\n",
    "resolution = 8 * 2 ** step\n",
    "loader = sample_data(\n",
    "    args.path, args.batch.get(resolution, args.batch_default), resolution\n",
    ")\n",
    "data_loader = iter(loader)\n",
    "\n",
    "adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))\n",
    "adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))\n",
    "\n",
    "pbar = tqdm(range(500_000))\n",
    "\n",
    "requires_grad(generator, False)\n",
    "requires_grad(discriminator, True)\n",
    "\n",
    "disc_loss_val = 0\n",
    "gen_loss_val = 0\n",
    "grad_loss_val = 0\n",
    "\n",
    "alpha = 0\n",
    "used_sample = 0\n",
    "# used_sample = 9972*16\n",
    "\n",
    "n_class_age = 6\n",
    "n_repeat = 2\n",
    "y_onehot = torch.FloatTensor(args.batch_default, n_class_age)\n",
    "\n",
    "\n",
    "print ('Resolution: ', resolution, '|Step: ', step, '|Batch_size: ', args.batch.get(resolution, args.batch_default), ' |Generator lr: ', \n",
    "      g_optimizer.state_dict()['param_groups'][0]['lr'], ' |Style lr: ', g_optimizer.state_dict()['param_groups'][1]['lr'])\n",
    "for i in pbar:\n",
    "    discriminator.zero_grad()\n",
    "\n",
    "    alpha = min(1, 1 / args.phase * (used_sample + 1))\n",
    "\n",
    "    if (i+1)%10000==0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'generator': generator.module.state_dict(),\n",
    "                'discriminator': discriminator.module.state_dict(),\n",
    "                'g_optimizer': g_optimizer.state_dict(),\n",
    "                'd_optimizer': d_optimizer.state_dict(),\n",
    "            },\n",
    "            f'checkpoint/train_step-{step}-{i}-{alpha}.model',\n",
    "        )\n",
    "\n",
    "    if used_sample > args.phase * 2 and step < (int(math.log2(args.max_size)) - 3):\n",
    "        step += 1\n",
    "\n",
    "        if step > int(math.log2(args.max_size)) - 3:\n",
    "            step = int(math.log2(args.max_size)) - 3\n",
    "\n",
    "        else:\n",
    "            alpha = 0\n",
    "            used_sample = 0\n",
    "\n",
    "        resolution = 8 * 2 ** step\n",
    "\n",
    "        loader = sample_data(\n",
    "            args.path, args.batch.get(resolution, args.batch_default), resolution\n",
    "        )\n",
    "        data_loader = iter(loader)\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                'generator': generator.module.state_dict(),\n",
    "                'discriminator': discriminator.module.state_dict(),\n",
    "                'g_optimizer': g_optimizer.state_dict(),\n",
    "                'd_optimizer': d_optimizer.state_dict(),\n",
    "            },\n",
    "            f'checkpoint/train_step-{step}.model',\n",
    "        )\n",
    "\n",
    "        adjust_lr(g_optimizer, args.lr.get(resolution, 0.001))\n",
    "        adjust_lr(d_optimizer, args.lr.get(resolution, 0.001))\n",
    "        print ('Resolution: ', resolution, '|Step: ', step, '|Batch_size: ', args.batch.get(resolution, args.batch_default), ' |Generator lr: ', \n",
    "              g_optimizer.state_dict()['param_groups'][0]['lr'], ' |Style lr: ', g_optimizer.state_dict()['param_groups'][1]['lr'])\n",
    "\n",
    "    try:\n",
    "        real_image, label = next(data_loader)\n",
    "\n",
    "    except (OSError, StopIteration):\n",
    "        data_loader = iter(loader)\n",
    "        real_image, label = next(data_loader)\n",
    "\n",
    "    used_sample += real_image.shape[0]\n",
    "\n",
    "    b_size = real_image.size(0)\n",
    "    real_image = real_image.cuda()\n",
    "    y_onehot.zero_()\n",
    "\n",
    "    label = label.unsqueeze(1)\n",
    "\n",
    "    if y_onehot.shape[0] == b_size:\n",
    "        label = y_onehot.scatter_(1, label, 1)\n",
    "    else:\n",
    "        y_onehot_temp = torch.FloatTensor(b_size, n_class_age)\n",
    "        y_onehot_temp.zero_()\n",
    "        label = y_onehot_temp.scatter_(1, label, 1)\n",
    "\n",
    "    label = label.cuda()\n",
    "    label_age = label.clone()\n",
    "    label = label.repeat(1,n_repeat)\n",
    "\n",
    "    if args.loss == 'wgan-gp':\n",
    "        real_predict = discriminator(real_image, label_age=label_age, step=step, alpha=alpha)\n",
    "        real_predict = real_predict.mean() - 0.001 * (real_predict ** 2).mean()\n",
    "        (-real_predict).backward()\n",
    "\n",
    "    elif args.loss == 'r1':\n",
    "        real_image.requires_grad = True\n",
    "        real_predict = discriminator(real_image, label_age=label_age, step=step, alpha=alpha)\n",
    "        real_predict = F.softplus(-real_predict).mean()\n",
    "        real_predict.backward(retain_graph=True)\n",
    "\n",
    "        grad_real = grad(\n",
    "            outputs=real_predict.sum(), inputs=real_image, create_graph=True\n",
    "        )[0]\n",
    "        grad_penalty = (\n",
    "            grad_real.view(grad_real.size(0), -1).norm(2, dim=1) ** 2\n",
    "        ).mean()\n",
    "        grad_penalty = 10 / 2 * grad_penalty\n",
    "        grad_penalty.backward()\n",
    "        grad_loss_val = grad_penalty.item()\n",
    "\n",
    "    if args.mixing and random.random() < 0.9 and False:\n",
    "        gen_in11, gen_in12, gen_in21, gen_in22 = torch.randn(\n",
    "            4, b_size, code_size - n_class_age*n_repeat, device='cuda'\n",
    "        ).chunk(4, 0)\n",
    "        gen_in1 = [gen_in11.squeeze(0), gen_in12.squeeze(0)]\n",
    "        gen_in2 = [gen_in21.squeeze(0), gen_in22.squeeze(0)]\n",
    "\n",
    "    else:\n",
    "        gen_in1, gen_in2 = torch.randn(2, b_size, code_size - n_class_age*n_repeat, device='cuda').chunk(2, 0)            \n",
    "        gen_in1 = gen_in1.squeeze(0)\n",
    "        gen_in2 = gen_in2.squeeze(0)\n",
    "\n",
    "    gen_in1 = torch.cat((label, gen_in1), 1)\n",
    "    gen_in2 = torch.cat((label, gen_in2), 1)\n",
    "\n",
    "    fake_image = generator(gen_in1, step=step, alpha=alpha)\n",
    "    fake_predict = discriminator(fake_image, label_age=label_age, step=step, alpha=alpha)\n",
    "\n",
    "    if args.loss == 'wgan-gp':\n",
    "        fake_predict = fake_predict.mean()\n",
    "        fake_predict.backward()\n",
    "\n",
    "        eps = torch.rand(b_size, 1, 1, 1).cuda()\n",
    "        x_hat = eps * real_image.data + (1 - eps) * fake_image.data\n",
    "        x_hat.requires_grad = True\n",
    "        hat_predict = discriminator(x_hat, label_age=label_age, step=step, alpha=alpha)\n",
    "        grad_x_hat = grad(\n",
    "            outputs=hat_predict.sum(), inputs=x_hat, create_graph=True\n",
    "        )[0]\n",
    "        grad_penalty = (\n",
    "            (grad_x_hat.view(grad_x_hat.size(0), -1).norm(2, dim=1) - 1) ** 2\n",
    "        ).mean()\n",
    "        grad_penalty = 10 * grad_penalty\n",
    "        grad_penalty.backward()\n",
    "        grad_loss_val = grad_penalty.item()\n",
    "        disc_loss_val = (real_predict - fake_predict).item()\n",
    "\n",
    "    elif args.loss == 'r1':\n",
    "        fake_predict = F.softplus(fake_predict).mean()\n",
    "        fake_predict.backward()\n",
    "        disc_loss_val = (real_predict + fake_predict).item()\n",
    "\n",
    "    d_optimizer.step()\n",
    "\n",
    "    if (i + 1) % n_critic == 0:\n",
    "        generator.zero_grad()\n",
    "\n",
    "        requires_grad(generator, True)\n",
    "        requires_grad(discriminator, False)\n",
    "\n",
    "        fake_image = generator(gen_in2, step=step, alpha=alpha)\n",
    "\n",
    "        predict = discriminator(fake_image, label_age=label_age, step=step, alpha=alpha)\n",
    "\n",
    "        if args.loss == 'wgan-gp':\n",
    "            loss = -predict.mean()\n",
    "\n",
    "        elif args.loss == 'r1':\n",
    "            loss = F.softplus(-predict).mean()\n",
    "\n",
    "        gen_loss_val = loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        requires_grad(generator, False)\n",
    "        requires_grad(discriminator, True)\n",
    "\n",
    "    if (i + 1) % 500 == 0:\n",
    "        images = []\n",
    "\n",
    "        gen_i, gen_j = args.gen_sample.get(resolution, (n_class_age, 10))\n",
    "        test_onehot = torch.FloatTensor(gen_j, n_class_age)\n",
    "        random_z = torch.randn(gen_j, code_size - n_class_age*n_repeat).cuda()\n",
    "        with torch.no_grad():\n",
    "            for age_code in range(gen_i):\n",
    "                temp_age = torch.tensor([[age_code]]*gen_j)\n",
    "\n",
    "                test_onehot.zero_()\n",
    "                label_test_age = test_onehot.scatter_(1, temp_age, 1).cuda()\n",
    "                label_test_age = label_test_age.repeat(1,n_repeat)\n",
    "                gen_test = torch.cat((label_test_age, random_z), 1)\n",
    "                images.append(generator(gen_test, step=step, alpha=alpha))\n",
    "        utils.save_image(\n",
    "            torch.cat(images, 0),\n",
    "            f'sample/{str(i + 1).zfill(6)}.png',\n",
    "            nrow=gen_j,\n",
    "            normalize=True,\n",
    "            range=(-1, 1),\n",
    "        )\n",
    "\n",
    "    state_msg = (\n",
    "        f'Size: {8 * 2 ** step}; G: {gen_loss_val:.3f}; D: {disc_loss_val:.3f};'\n",
    "        f' Grad: {grad_loss_val:.3f}; Alpha: {alpha:.5f}'\n",
    "    )\n",
    "\n",
    "    pbar.set_description(state_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
